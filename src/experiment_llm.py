from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.callbacks import get_openai_callback 
import pandas as pd
from tqdm import tqdm
from typing import Tuple
from extract_final_answer import extract_answer_gsm8k, extract_ai_answer_aqua, extract_true_answer_aqua, extract_ai_answer_strategyqa, extract_true_answer_strategyqa
from generate_fewshot_demonstrations import generate_severalcontexts_fewshot_random_demonstrations
from save_load_results import save_results_different_contexts, save_results_same_contexts
import copy
import random
from constant_variables import COST_PER_TOKEN, ESTIMATE_COMPLETION_TOKENS, EXTRACT_ANSWERS_DIC, PREFIX_DIC, SUFFIX_DIC, CONTEXT_FUNC_DIC
from generate_fewshot_demonstrations import generate_singlecontext_fewshot_random_demonstrations_gsm8k

def predict_llm(template: str, question: str, model_name: str) -> Tuple[str, int]:
    """
        Run a LLMChain for given a prompt template and question. Return the completion and
        total nr of processed tokens during the run.
        
        Args: 
            template (str): template which includes prefix, few-shot demonstrations and suffix
            question (str): question which will be passed to LLM
            model_name (str): the id/name of the LLM
        Returns:
            result (str): completion generated by the LLM
            cb.total_tokens (int): nr of processed tokens during the run

    """
    prompt = PromptTemplate(input_variables=["question"], template=template)
    llm = OpenAI(model_name=model_name, temperature=0)
    llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)
    with get_openai_callback() as cb:
        result = llm_chain.run(question)
        
    return result, cb.total_tokens, cb.total_cost


def several_runs_same_context_for_run(seeds: list, dataset_name: str, dataset: list, strategy: str, model_name: str, nr_examples: int):
    """
        Run an experiment for a given strategy and dataset.
        1) Generate different contexts which contain few-shot demonstrations and 
        2) Run an LLM on all examples for a given context
        3) Store the true answer and completions generated by LLM.

        Args:
            seeds (list): a list of seeds which are used to sample few-shot demonstrations
            dataset_name (str): name of the dataset
            dataset (list): a list of examples from the dataset
            strategy (str): strategy should be 'standard' or 'cot'
            model_name (str): the model name from OpenAI
            nr_examples (int): the number of few-shot demonstrations
    """
    
    suffix_data = SUFFIX_DIC[dataset_name][strategy]
    if dataset_name == 'aqua':
        suffix_subset = suffix_data['subset']
        suffix_question = suffix_data['suffix']
    else:
        suffix = suffix_data
        
    
    prefix = PREFIX_DIC[dataset_name][strategy]

    extract_true_answer_func = EXTRACT_ANSWERS_DIC[dataset_name]['extract_true_answer_func']
    extract_ai_answer_func = EXTRACT_ANSWERS_DIC[dataset_name]['extract_ai_answer_func']

    list_contexts, no_test_examples = generate_severalcontexts_fewshot_random_demonstrations(seeds, dataset_name, dataset, nr_examples, strategy)
    dataframes_dic = {}
    
    if dataset_name == 'strategyqa' and len(list_contexts) == 1:
        seeds = ['no_seed']

    for seed, context in tqdm(zip(seeds, list_contexts)):
        df = pd.DataFrame()
        
        processed_dataset = copy.deepcopy(dataset)
        for to_remove_example in no_test_examples[seed]:
            processed_dataset.remove(to_remove_example)

        for example in processed_dataset:
            # build the suffix
            if dataset_name == 'aqua':                
                formatted_suffix_subset = suffix_subset.format(example['options'])
                suffix = suffix_question + formatted_suffix_subset
            
            # build the template using prefix, context and suffix
            template = prefix + context + suffix
            ai_completion, token_count, total_cost = predict_llm(template, example['question'], model_name)
            
            # extract the ai final answer from the model completion
            ai_final_answer = extract_ai_answer_func(ai_completion)
            # extract the true final answer
            true_final_answer = extract_true_answer_func(example['answer'])

            row_dic = {'question' : [example['question']],
                       'true_final_answer' : [true_final_answer],
                       'ai_final_answer' : [ai_final_answer],
                       'total_tokens' : [token_count],
                       'total_price' : [total_cost],
                       }
            
            if strategy == 'cot':
                row_dic['true_completion'] = [example['answer']]
                row_dic['ai_completion'] = [ai_completion]


            df = pd.concat([df, pd.DataFrame(row_dic)], ignore_index=False)
                
        
        dataframes_dic[seed] = [df, context]
    
    save_results_same_contexts(dataset_name, dataframes_dic, strategy)
    
    print('Experiment Run Successfully')


def single_run_different_contexts(seeds: list, dataset_name: str, dataset: list, strategy: str, model_name: str, nr_examples: int):
    """
        Run an experiment for a given strategy and dataset.
        1) Generate different contexts which contain few-shot demonstrations and 
        2) Run an LLM on all examples for a given context
        3) Store the true answer and completions generated by LLM.

        Args:
            seeds (list): a list of seeds which are used to sample few-shot demonstrations
            dataset_name (str): name of the dataset
            dataset (list): a list of examples from the dataset
            strategy (str): strategy should be 'standard' or 'cot'
            model_name (str): the model name from OpenAI
            nr_examples (int): the number of few-shot demonstrations
    """
    
    suffix_data = SUFFIX_DIC[dataset_name][strategy]
    if dataset_name == 'aqua':
        suffix_subset = suffix_data['subset']
        suffix_question = suffix_data['suffix']
    else:
        suffix = suffix_data

    context_func = CONTEXT_FUNC_DIC[dataset_name]

    prefix = PREFIX_DIC[dataset_name][strategy]

    extract_true_answer_func = EXTRACT_ANSWERS_DIC[dataset_name]['extract_true_answer_func']
    extract_ai_answer_func = EXTRACT_ANSWERS_DIC[dataset_name]['extract_ai_answer_func']

    df = pd.DataFrame()
    for seed, example in zip(seeds[:3], dataset[:3]):
        # exclude the current test example from few-shot demonstrations samples
        processed_dataset = copy.deepcopy(dataset)
        processed_dataset.remove(example)

        # sample few-shot demonstrations
        random.seed(seed)
        selected_examples = random.sample(processed_dataset, nr_examples)

        context = context_func(selected_examples, strategy)

        # build the suffix
        if dataset_name == 'aqua':                
            formatted_suffix_subset = suffix_subset.format(example['options'])
            suffix = suffix_question + formatted_suffix_subset
        
        # build the template using prefix, context and suffix
        template = prefix + context + suffix
        ai_completion, token_count, total_cost = predict_llm(template, example['question'], model_name)
        
        # extract the final answer from the model completion
        ai_final_answer = extract_ai_answer_func(ai_completion)
        # extract the true answer
        true_final_answer = extract_true_answer_func(example['answer'])
        

        row_dic = {'question' : [example['question']],
                       'true_final_answer' : [true_final_answer],
                       'ai_final_answer' : [ai_final_answer],
                       'total_tokens' : [token_count],
                       'total_price' : [total_cost],
                       }
            
        if strategy == 'cot':
            row_dic['true_completion'] = [example['answer']]
            row_dic['ai_completion'] = [ai_completion]

        df = pd.concat([df, pd.DataFrame(row_dic)], ignore_index=False)
    
    print('Experiment Run Successfully!')
    return df

