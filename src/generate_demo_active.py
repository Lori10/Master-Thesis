# This file used to generate uncertainty score for each question
from utils import *
import time
import argparse
import numpy as np
import json
from scipy.stats import entropy
from typing import Tuple
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.callbacks import get_openai_callback 

def predict_llm(template: str, model:str, question:str, max_tokens: int, time_interval, temperature=0.7, stop=None) -> Tuple[str, int]:
    """
        Run a LLMChain for given a prompt template and question. Return the completion and
        total nr of processed tokens during the run.
        
        Args: 
            template (str): template which includes prefix, few-shot demonstrations and suffix
            question (str): question which will be passed to LLM
            model_name (str): the id/name of the LLM
        Returns:
            result (str): completion generated by the LLM
            cb.total_tokens (int): nr of processed tokens during the run

    """
    prompt = PromptTemplate(input_variables=["question"], template=template)
    # llm = OpenAI(model_name=model, temperature=temperature, max_tokens=max_tokens, stop=stop, 
    #              top_p=1, frequency_penalty=0, presence_penalty=0)

    llm = OpenAI(model_name=model, temperature=temperature)
    llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=False)
    with get_openai_callback() as cb:
        result = llm_chain.run(question)
        
    return result, cb.total_tokens, cb.total_cost


def main():
    args = arg_parser()
    print('*****************************')
    print(args)
    print('*****************************')

    print(f"API_KEY: {API_KEY}")

    if not os.path.exists(args.demos_save_dir):
        os.makedirs(args.demos_save_dir)
        os.makedirs(args.demos_save_dir + 'active_cot')
        os.makedirs(args.demos_save_dir + 'active_cot/' + args.dataset)
    elif not os.path.exists(args.demos_save_dir + 'active_cot'):
        os.makedirs(args.demos_save_dir + 'active_cot')
        os.makedirs(args.demos_save_dir + 'active_cot/' + args.dataset)
    elif not os.path.exists(args.demos_save_dir + 'active_cot/' + args.dataset):
        os.makedirs(args.demos_save_dir + 'active_cot/' + args.dataset)

    args.demos_save_dir = f"{args.demos_save_dir}/active_cot/{args.dataset}/"

    set_random_seed(args.random_seed)

    dataloader = create_dataloader(args)

    if args.dataset_size > args.qet_limit:
        dataloader = dataloader[:args.qes_limit] # replace 7 with 1000; only take 1000 questions randomly to annotate, randomness decided by seed
    print(f"Dataloader size: {len(dataloader)}")


    if args.qes_limit == 0:
        args.qes_limit = len(dataloader)

    start =time.time()
    result = create_uncertainty(args, dataloader)
    end = time.time()
    print('Total Execution Time: ', end - start, " seconds")

    demos = {'demo': result[:args.nr_demos]}
    with open(f"{args.demos_save_dir}demos_k_{args.num_trails}", 'w', encoding="utf-8") as write_f:
        json.dump(demos, write_f, indent=4, ensure_ascii=False)

    uncertainty_estimation_dir = f"{args.demos_save_dir}active/uncertainty_estimation/"
    if not os.path.exists(uncertainty_estimation_dir):
        os.makedirs(uncertainty_estimation_dir)
        
    with open(f"{uncertainty_estimation_dir}{args.dataset}_k_{args.num_trails}.txt", 'w') as f:
        try:
            f.write(json.dumps(result, indent=4))
        except:
            for item in result:
                try:
                    if args.dataset in ("gsm8k", "asdiv", "svamp", "singleeq", "addsub", "multiarith"):
                        f.write(f"{item}, uncertainty: {len(item[-1])}, variance: {item[1]}\n")
                    else:
                        f.write(f"{item}, uncertainty: {len(item[-1])}\n")
                except:
                    pass


def generate_uncertainty_qes(args, example):
    if args.method == "few_shot_cot":
        given_prompt_list = create_several_input_prompts(args, cot_flag=True)
        assert len(given_prompt_list) == 1
        given_prompt = given_prompt_list[0]

    if args.dataset in ("gsm8k", "asdiv", "svamp", "singleeq", "addsub", "multiarith"):
        # the float is reserved for variance calculation result
        uncertainty_record = {'dataset_idx':example['question_idx'], 'question': example['question'],
                              'rationale': example['rationale'], 'final_answer': example['final_answer'] , 
                              'variance':float, 'entropy':float, 'occurrence':{}}
    elif args.dataset == "strategyqa":
        uncertainty_record = {'dataset_idx': example['question_idx'], 'question': example['question'],
                              'rationale': example['rationale'], 'final_answer': example['final_answer'],
                             'entropy':float, 'occurrence':{"yes":0, "no":0}}
    else:
        uncertainty_record = {'dataset_idx':example['question_idx'], 'question': example['question'],
                              'rationale': example['rationale'], 'final_answer': example['final_answer'],
                              'entropy':float, 'occurrence':{}}

    for trail in range(args.num_trails):
        # if zero-shot to generate uncertainty, construct first stage zero-shot prompt (step by step)
        if args.method == "few_shot_cot":
            prompt = given_prompt + "Q: " + "{question}" + "\nA: Let's think step by step."
        elif args.method == "zero_shot_cot":
            prompt = "Q: " + "{question}" + "\nA: Let's think step by step."
        
        # if use zero-shot, here we get the first stage zero-shot result
        # if not use zero-shot, here we get the final output
        # responses = ChatGPT3_request(model=args.model, input_prompt=prompt_list, max_tokens=args.max_length_cot, time_interval=args.api_time_interval
        #                          , temperature=args.temperature , stop=['Question:', "Q:"])
        responses, _, _ = predict_llm(template=prompt, question=example['question'], model="gpt-3.5-turbo",
                                      max_tokens=args.max_length_cot, time_interval=args.api_time_interval, temperature=args.temperature, stop=['Question:', "Q:"]) 

        # construct second stage prompt, to generate a single arabic num answer
        # if args.method == "zero_shot_cot":
        #     prompt_list[0] += responses['choices'][0]['text'] + args.direct_answer_trigger

        #     # get the second stage zero-shot rationale result -> arabic num answer
        #     responses = GPT3_request(model=args.model, input_prompt=prompt_list, max_tokens=args.max_length_cot, time_interval=args.api_time_interval,
        #                               temperature=args.temperature, stop='.')

        # extract the pred answer
        pred_ans = answer_extraction(args, responses)
        print(f'Single Trial Prediction: {pred_ans}\n')

        # check uncertainty
        if pred_ans != "":
            if pred_ans in uncertainty_record['occurrence']:
                uncertainty_record['occurrence'][pred_ans] += 1 # increment answer occurrence
            else:
                uncertainty_record['occurrence'][pred_ans] = 1 # first occurence
        else:
            # Handle no solution case
            if NO_SOLUTION in uncertainty_record['occurrence']:
                uncertainty_record['occurrence'][NO_SOLUTION] += 1
            else:
                uncertainty_record['occurrence'][NO_SOLUTION] = 1

    # calculate the variance for the question (only applied to datasets with numerical answer)
    if args.dataset in ("gsm8k", "asdiv", "svamp", "singleeq", "addsub", "multiarith"):
        ans_list = []
        for ans, occurs in uncertainty_record['occurrence'].items():
            for i in range(int(occurs)):
                ans_list.append(float(ans))
        uncertainty_record['variance'] = np.var(ans_list)
        
    # calculate the entropy for all dataset
    frequency_list = list(uncertainty_record['occurrence'].values())
    uncertainty_record['entropy'] = entropy(frequency_list)

    # calculate the disagreement for all dataset
    uncertainty_record['disagreement'] = len(uncertainty_record['occurrence'])
    
    return uncertainty_record


# return a sorted list by uncertainty from high to low
def create_uncertainty(args, dataloader):
    result = []
    count = 0

    for example in dataloader:
        if count == args.qes_limit:
            break

        print(f'Question: {example["question"]}\n')
        uncertainty_record = generate_uncertainty_qes(args, example)
        print(f'Uncertainty Record: {uncertainty_record}')
        result.append(uncertainty_record)
        count += 1

    if args.sort_by == "disagreement":
        if args.dataset == "strategyqa":
            try:
                # sort based on the entropy or the difference between yes and no answers
                result.sort(key=lambda x: abs(x['occurrence']['yes'] - x['occurrence']['no']))
            except:
                # sort by disagreement
                result.sort(key=lambda x: -len(x['occurrence']))
        else:
            result.sort(key=lambda x: -len(x['occurrence']))
    elif args.sort_by == "variance" and args.dataset in ("gsm8k", "asdiv", "svamp", "singleeq", "addsub", "multiarith"):
        # sort by variance
        result.sort(key=lambda x: -x['variance'])
    elif args.sort_by == "entropy" :
        result.sort(key=lambda x:-x['entropy'])
    return result

def arg_parser():
    parser = argparse.ArgumentParser(description="Active_CoT")
    parser.add_argument("--random_seed", type=int, default=42, help="random seed")
    parser.add_argument(
        "--dataset", type=str, default="gsm8k", choices=["gsm8k","svamp", "aqua", "csqa", "last_letters", "strategyqa", "asdiv", "singleeq", "addsub", "multiarith"], help="dataset to inference"
    )
    parser.add_argument(
        "--dir_prompts", type=str, default="prompts_active", help="prompts to use"
    )

    parser.add_argument(
        "--demos_save_dir", type=str, default="demos/", help="maximum number of reasoning chains"
    )

    parser.add_argument(
        "--model", type=str, default="text-davinci-002", choices=["text-davinci-002", "code-davinci-002"], help="model used for decoding."
    )
    parser.add_argument(
        "--method", type=str, default="few_shot_cot", choices=["zero_shot_cot", "few_shot_cot"], help="method"
    )
    parser.add_argument(
        "--max_length_cot", type=int, default=256, help="maximum length of output tokens by model for reasoning extraction"
    )
    parser.add_argument(
        "--qes_limit", type=int, default=50, help="whether to limit training dataset size. if 0, the dataset size is unlimited and we use all the samples in the dataset for creating the demonstrations."
    )
    parser.add_argument(
        "--api_time_interval", type=float, default=1.0, help="how many seconds sleep between each request"
    )
    parser.add_argument(
        "--temperature", type=float, default=0.7, help=""
    )
    parser.add_argument(
        "--num_trails", type=int, default=5, help="number of trails to run for each qeestion"
    )
    parser.add_argument(
        "--sort_by", type=str, default='disagreement', choices=['disagreement', 'variance', 'entropy'], help="sort the final result by given option"
    )
    parser.add_argument(
        "--concat_length", type=int, default=2, help='Used for task last_letters, indicates length of last letter concat'
    )

    parser.add_argument(
        "--nr_demos", type=int, default=7, help='number of demonstrations'
    )

    
    args = parser.parse_args()
    
    # Fill in the dataset path
    if args.dataset == "gsm8k":
        args.data_path = "../datasets/gsm8k/train.jsonl"
        args.direct_answer_trigger = "\nTherefore, the answer (arabic numerals) is"
    elif args.dataset == "aqua":
        args.data_path = "../datasets/AQuA/train.json" 
        args.direct_answer_trigger = "\nThe answer is"

    elif args.dataset == "svamp":
        args.direct_answer_trigger = "\nTherefore, the answer (arabic numerals) is"
    elif args.dataset == "asdiv":
        args.direct_answer_trigger = "\nTherefore, the answer (arabic numerals) is"
    elif args.dataset == "singleeq":
        args.direct_answer_trigger = "\nTherefore, the answer (arabic numerals) is"
    elif args.dataset == "addsub":
        args.direct_answer_trigger = "\nTherefore, the answer (arabic numerals) is"
    elif args.dataset == "multiarith":
        args.direct_answer_trigger = "\nTherefore, the answer (arabic numerals) is"
    
    elif args.dataset == "csqa":
        args.dataset_path = "./dataset/CSQA/train_rand_split.jsonl" # train data path
        args.direct_answer_trigger = "\nSo the answer is"
    elif args.dataset == "strategyqa":
        args.dataset_path = "./dataset/strategyQA/train.json" # train data path
        args.direct_answer_trigger = "\nTherefore, the answer (Yes or No) is"
    elif args.dataset == "last_letters":
        args.dataset_path = "./dataset/last_letters/last_letters_train2.json" # train data path
        args.direct_answer_trigger = "\nTherefore, the answer is"
    else:
        raise ValueError("dataset is not properly defined ...")
        
    # "Therefore, the answer ..." -> "The answer ..."
    trigger = args.direct_answer_trigger.replace("\nTherefore, ", "")
    args.direct_answer_trigger_for_zeroshot = trigger[0].upper() + trigger[1:]
    args.direct_answer_trigger_for_zeroshot_cot = args.direct_answer_trigger
    args.direct_answer_trigger_for_fewshot = "The answer is"
    args.cot_trigger = "Let's think step by step."
    
    return args


if __name__ == "__main__":
    main()